{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Eshanka\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "import os\n",
    "import nltk\n",
    "import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import string\n",
    "import keras\n",
    "import keras.backend as K\n",
    "import tensorflow as tf\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "nltk.download(\"punkt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import Callback, EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from keras.engine import Layer\n",
    "from keras.layers import Activation, Add, Bidirectional, Conv1D, Dense, Dropout, Embedding, Flatten, LeakyReLU\n",
    "from keras.layers import concatenate, GRU, Input, LSTM, MaxPooling1D\n",
    "from keras.layers import GlobalAveragePooling1D,  GlobalMaxPooling1D, SpatialDropout1D\n",
    "from keras.models import Model, Sequential\n",
    "from keras.optimizers import Adam\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing import text, sequence\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, log_loss\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras import initializers, regularizers, constraints, optimizers, layers, callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3.0\n",
      "2.4.3\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)\n",
    "print(keras.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting up parameter globals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "gru_len = 256\n",
    "Routings = 5\n",
    "Num_capsule = 2\n",
    "Dim_capsule = 16\n",
    "dropout_p = 0.3\n",
    "rate_drop_dense = 0.3\n",
    "\n",
    "batch_size = 128\n",
    "recurrent_units = 64\n",
    "dropout_rate = 0.3 \n",
    "dense_size = 20\n",
    "sentences_length = 50\n",
    "fold_count = 10\n",
    "max_features = 20000\n",
    "maxlen = 1000\n",
    "embed_size = 256"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Non-linear activation fucntion for capsule layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def squash(x, axis=-1):\n",
    "    s_squared_norm = K.sum(K.square(x), axis, keepdims=True)\n",
    "    scale = K.sqrt(s_squared_norm + K.epsilon())\n",
    "    return x / scale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generating the capsule network algorithm with slight tweaks to optimize with the dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CapsuleLayer(layers.Layer):\n",
    "    \"\"\"\n",
    "    The capsule layer. It is similar to Dense layer. Dense layer has `in_num` inputs, each is a scalar, the output of the \n",
    "    neuron from the former layer, and it has `out_num` output neurons. CapsuleLayer just expand the output of the neuron\n",
    "    from scalar to vector. So its input shape = [None, input_num_capsule, input_dim_capsule] and output shape = \\\n",
    "    [None, num_capsule, dim_capsule]. For Dense Layer, input_dim_capsule = dim_capsule = 1.\n",
    "    \n",
    "    :param num_capsule: number of capsules in this layer\n",
    "    :param dim_capsule: dimension of the output vectors of the capsules in this layer\n",
    "    :param routings: number of iterations for the routing algorithm\n",
    "    \"\"\"\n",
    "    def __init__(self, num_capsule, dim_capsule, routings=3,\n",
    "                 kernel_initializer='glorot_uniform',\n",
    "                 **kwargs):\n",
    "        super(CapsuleLayer, self).__init__(**kwargs)\n",
    "        self.num_capsule = num_capsule\n",
    "        self.dim_capsule = dim_capsule\n",
    "        self.routings = routings\n",
    "        self.kernel_initializer = initializers.get(kernel_initializer)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) >= 3, \"The input Tensor should have shape=[None, input_num_capsule, input_dim_capsule]\"\n",
    "        self.input_num_capsule = input_shape[1]\n",
    "        self.input_dim_capsule = input_shape[2]\n",
    "\n",
    "        # Transform matrix\n",
    "        self.W = self.add_weight(shape=[self.num_capsule, self.input_num_capsule,\n",
    "                                        self.dim_capsule, self.input_dim_capsule],\n",
    "                                 initializer=self.kernel_initializer,\n",
    "                                 name='W')\n",
    "\n",
    "        self.built = True\n",
    "\n",
    "    def call(self, inputs, training=None):\n",
    "        # Expand the input in axis=1, tile in that axis to num_capsule, and \n",
    "        # expands another axis at the end to prepare the multiplication with W.\n",
    "        #  inputs.shape=[None, input_num_capsule, input_dim_capsule]\n",
    "        #  inputs_expand.shape=[None, 1, input_num_capsule, input_dim_capsule]\n",
    "        #  inputs_tiled.shape=[None, num_capsule, input_num_capsule, \n",
    "        #                            input_dim_capsule, 1]\n",
    "        inputs_expand = tf.expand_dims(inputs, 1)\n",
    "        inputs_tiled  = tf.tile(inputs_expand, [1, self.num_capsule, 1, 1])\n",
    "        inputs_tiled  = tf.expand_dims(inputs_tiled, 4)\n",
    "\n",
    "        # Compute `W * inputs` by scanning inputs_tiled on dimension 0 (map_fn).\n",
    "        # - Use matmul (without transposing any element). Note the order!\n",
    "        # Thus:\n",
    "        #  x.shape=[num_capsule, input_num_capsule, input_dim_capsule, 1]\n",
    "        #  W.shape=[num_capsule, input_num_capsule, dim_capsule,input_dim_capsule]\n",
    "        # Regard the first two dimensions as `batch` dimension,\n",
    "        # then matmul: [dim_capsule, input_dim_capsule] x [input_dim_capsule, 1]-> \n",
    "        #              [dim_capsule, 1].\n",
    "        #  inputs_hat.shape=[None, num_capsule, input_num_capsule, dim_capsule, 1]\n",
    "        \n",
    "        inputs_hat = tf.map_fn(lambda x: tf.matmul(self.W, x), elems=inputs_tiled)     \n",
    "\n",
    "        # Begin: Routing algorithm ----------------------------------------------#\n",
    "        # The prior for coupling coefficient, initialized as zeros.\n",
    "        #  b.shape = [None, self.num_capsule, self.input_num_capsule, 1, 1].\n",
    "        b = tf.zeros(shape=[tf.shape(inputs_hat)[0], self.num_capsule, \n",
    "                            self.input_num_capsule, 1, 1])\n",
    "\n",
    "        assert self.routings > 0, 'The routings should be > 0.'\n",
    "        for i in range(self.routings):\n",
    "            # Apply softmax to the axis with `num_capsule`\n",
    "            #  c.shape=[batch_size, num_capsule, input_num_capsule, 1, 1]\n",
    "            c = layers.Softmax(axis=1)(b)\n",
    "\n",
    "            # Compute the weighted sum of all the predicted output vectors.\n",
    "            #  c.shape =  [batch_size, num_capsule, input_num_capsule, 1, 1]\n",
    "            #  inputs_hat.shape=[None, num_capsule, input_num_capsule,dim_capsule,1]\n",
    "            # The function `multiply` will broadcast axis=3 in c to dim_capsule.\n",
    "            #  outputs.shape=[None, num_capsule, input_num_capsule, dim_capsule, 1]\n",
    "            # Then sum along the input_num_capsule\n",
    "            #  outputs.shape=[None, num_capsule, 1, dim_capsule, 1]\n",
    "            # Then apply squash along the dim_capsule\n",
    "            outputs = tf.multiply(c, inputs_hat)\n",
    "            outputs = tf.reduce_sum(outputs, axis=2, keepdims=True)\n",
    "            outputs = squash(outputs, axis=-2)  # [None, 10, 1, 16, 1]\n",
    "\n",
    "            if i < self.routings - 1:\n",
    "                # Update the prior b.\n",
    "                #  outputs.shape =  [None, num_capsule, 1, dim_capsule, 1]\n",
    "                #  inputs_hat.shape=[None,num_capsule,input_num_capsule,dim_capsule,1]\n",
    "                # Multiply the outputs with the weighted_inputs (inputs_hat) and add  \n",
    "                # it to the prior b.  \n",
    "                outputs_tiled = tf.tile(outputs, [1, 1, self.input_num_capsule, 1, 1])\n",
    "                agreement = tf.matmul(inputs_hat, outputs_tiled, transpose_a=True)\n",
    "                b = tf.add(b, agreement)\n",
    "\n",
    "        # End: Routing algorithm ------------------------------------------------#\n",
    "        # Squeeze the outputs to remove useless axis:\n",
    "        #  From  --> outputs.shape=[None, num_capsule, 1, dim_capsule, 1]\n",
    "        #  To    --> outputs.shape=[None, num_capsule,    dim_capsule]\n",
    "        outputs = tf.squeeze(outputs, [2, 4])\n",
    "        return outputs\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return tuple([None, self.num_capsule, self.dim_capsule])\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {\n",
    "            'num_capsule': self.num_capsule,\n",
    "            'dim_capsule': self.dim_capsule,\n",
    "            'routings': self.routings\n",
    "        }\n",
    "        base_config = super(CapsuleLayer, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmv_acc(string_1):\n",
    "\n",
    "    string_1 = string_1.replace(\"ç\", \"c\")\n",
    "    string_1 = string_1.replace(\"Ç\", \"C\")\n",
    "    string_1 = string_1.replace(\"à\", \"a\")\n",
    "    string_1 = string_1.replace(\"Ä\", \"A\")\n",
    "    string_1 = string_1.replace(\"ä\", \"a\")\n",
    "    string_1 = string_1.replace(\"À\", \"A\")\n",
    "    string_1 = string_1.replace(\"Â\", \"A\")\n",
    "    string_1 = string_1.replace(\"â\", \"a\")\n",
    "    string_1 = string_1.replace(\"é\", \"e\")\n",
    "    string_1 = string_1.replace(\"è\", \"e\")\n",
    "    string_1 = string_1.replace(\"É\", \"E\")\n",
    "    string_1 = string_1.replace(\"È\", \"E\")\n",
    "    string_1 = string_1.replace(\"Ë\", \"E\")\n",
    "    string_1 = string_1.replace(\"ë\", \"e\")\n",
    "    string_1 = string_1.replace(\"Ê\", \"E\")\n",
    "    string_1 = string_1.replace(\"ê\", \"e\")\n",
    "    string_1 = string_1.replace(\"û\", \"u\")\n",
    "    string_1 = string_1.replace(\"Û\", \"U\")\n",
    "    string_1 = string_1.replace(\"ü\", \"u\")\n",
    "    string_1 = string_1.replace(\"Ü\", \"U\")\n",
    "    string_1 = string_1.replace(\"ï\", \"i\")\n",
    "    string_1 = string_1.replace(\"Ï\", \"I\")\n",
    "    string_1 = string_1.replace(\"î\", \"i\")\n",
    "    string_1 = string_1.replace(\"Î\", \"I\")\n",
    "    string_1 = string_1.replace(\"Ô\", \"O\")\n",
    "    string_1 = string_1.replace(\"ô\", \"o\")\n",
    "    string_1 = string_1.replace(\"Ö\", \"O\")\n",
    "    string_1 = string_1.replace(\"ö\", \"o\")\n",
    "    string_1 = string_1.replace(\"Ù\", \"U\")\n",
    "    string_1 = string_1.replace(\"ù\", \"u\")\n",
    "    string_1 = string_1.replace(\"ÿ\", \"y\")\n",
    "    string_1 = string_1.replace(\"æ\", \"ae\")\n",
    "    string_1 = string_1.replace(\"_\", \" \")\n",
    "    string_1 = string_1.replace(\"\\n\", \"\")\n",
    "\n",
    "    return string_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading dataset and Tokenizing the host urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dframe = pd.read_csv('Datasets/dga_data.csv', sep = ',')\n",
    "dframe = pd.DataFrame(dframe)\n",
    "df = dframe[[\"host\",\"isDGA\"]]\n",
    "domains = df['host'].apply(lambda x: rmv_acc(x))\n",
    "labels = df['isDGA']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_letters = set(' '.join([str(i) for i in domains]))\n",
    "all_letters.add(\"END\")\n",
    "len_letters = len(all_letters)\n",
    "char_map = {v: k for k, v in enumerate(all_letters)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(12,8))\n",
    "plt.hist([len(a) for a in domains], bins=36)\n",
    "plt.title(\"Length of the domains\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Embedding the dataset in a character level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = []\n",
    "y = []\n",
    "\n",
    "# Builds an empty line with a 1 at the index of character\n",
    "def set_flag(i):\n",
    "    tmp = np.zeros(len_letters)\n",
    "    tmp[i] = 1\n",
    "    return list(tmp)\n",
    "\n",
    "# Truncate names and create the matrix\n",
    "def prepare_X(X):\n",
    "    new_list = []\n",
    "    trunc_train_name = [str(i)[0:sentences_length] for i in X]\n",
    "\n",
    "    for i in trunc_train_name:\n",
    "        tmp = [set_flag(char_map[j]) for j in str(i)]\n",
    "        for k in range(0,sentences_length - len(str(i))):\n",
    "            tmp.append(set_flag(char_map[\"END\"]))\n",
    "        new_list.append(tmp)\n",
    "\n",
    "    return new_list\n",
    "\n",
    "\n",
    "X = prepare_X(domains.values)\n",
    "\n",
    "def prepare_y(y):\n",
    "    new_list = []\n",
    "    for i in y:\n",
    "        if i == 'dga':\n",
    "            new_list.append([1,0])\n",
    "        else:\n",
    "            new_list.append([0,1])\n",
    "\n",
    "    return new_list\n",
    "\n",
    "y = prepare_y(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spliting training and testing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Casting train and test lists into numpy arrays of type float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.asarray(X_train).astype('float32')\n",
    "X_test = np.asarray(X_test).astype('float32')\n",
    "y_train = np.asarray(y_train).astype('float32')\n",
    "y_test = np.asarray(y_test).astype('float32')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((107200, 50, 40), (107200, 2), (52800, 50, 40), (52800, 2))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape,y_train.shape,X_test.shape,y_test.shape\n",
    "#print(len_letters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DL model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(sequence_length, len_letters, dropout_rate, dense_size):\n",
    "    inputs = Input(shape=(sequence_length,len_letters,))\n",
    "    bi = Bidirectional(GRU(gru_len, activation='relu', dropout=dropout_rate, recurrent_dropout=dropout_rate, return_sequences=True),backward_layer=GRU(gru_len, return_sequences=True, go_backwards=True))(inputs)\n",
    "    capsule = CapsuleLayer(num_capsule=Num_capsule, dim_capsule=Dim_capsule)(bi)\n",
    "    capsule = Flatten()(capsule)\n",
    "    capsule = Dropout(dropout_rate)(capsule)\n",
    "    capsule = Dense(dense_size, activation='relu')(capsule)\n",
    "    capsule = Flatten()(capsule)\n",
    "    output = Dense(2, activity_regularizer=l2(0.002), activation='sigmoid')(capsule)\n",
    "    model = Model(inputs=inputs, outputs=output, name=\"CapsDGA\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(sequence_length, len_letters, dropout_rate, dense_size): \n",
    "    inputs = Input(shape=(sequence_length,))\n",
    "    embed_layer = Embedding(max_features,\n",
    "                                embed_size,\n",
    "                                input_length=sequence_length)(inputs)\n",
    "    embed_layer = SpatialDropout1D(rate_drop_dense)(embed_layer)\n",
    "\n",
    "    x = Bidirectional(GRU(gru_len,\n",
    "                        activation='relu',\n",
    "                        dropout=dropout_p,\n",
    "                        recurrent_dropout=dropout_p,\n",
    "                        return_sequences=True))(embed_layer)\n",
    "    capsule = CapsuleLayer(num_capsule=Num_capsule, dim_capsule=Dim_capsule,)(x)\n",
    "\n",
    "    capsule = Flatten()(capsule)\n",
    "    capsule = Dropout(dropout_p)(capsule)\n",
    "    capsule = LeakyReLU()(capsule)\n",
    "\n",
    "    x = Flatten()(x)\n",
    "    predictions = Dense(1, activation='sigmoid')(x)\n",
    "    model = Model(inputs=inputs, outputs=predictions, name=\"CapsDGA\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(sequence_length, len_letters, dropout_rate, dense_size):\n",
    "    model = Sequential()\n",
    "    model.add(Bidirectional(LSTM(512, return_sequences=True), backward_layer=LSTM(512, return_sequences=True, go_backwards=True), input_shape=(sentences_length,len_letters)))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Bidirectional(LSTM(512)))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(2, activity_regularizer=l2(0.002)))\n",
    "    model.add(Activation('softmax'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.compile(loss='categorical_crossentropy', optimizer='adam',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Monitoring and model saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#callback = EarlyStopping(monitor='val_loss', patience=5)\n",
    "mc = ModelCheckpoint('best_model_9.h5', monitor='val_loss', mode='min', verbose=1)\n",
    "reduce_lr_acc = ReduceLROnPlateau(monitor='val_accuracy', factor=0.1, patience=2, verbose=1, min_delta=1e-4, mode='max')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_size = 256\n",
    "# history = model.fit(X_train, y_train, batch_size=batch_size, epochs=35, verbose=1, validation_data =(X_test, y_test), callbacks=[mc, reduce_lr_acc])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting the model object with the layers loaded in and getting the summary on it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"CapsDGA\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 50, 40)]          0         \n",
      "_________________________________________________________________\n",
      "bidirectional (Bidirectional (None, 50, 512)           457728    \n",
      "_________________________________________________________________\n",
      "capsule_layer (CapsuleLayer) (None, 2, 16)             819200    \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 20)                660       \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 2)                 42        \n",
      "=================================================================\n",
      "Total params: 1,277,630\n",
      "Trainable params: 1,277,630\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "dgaModel = get_model(sentences_length, len_letters, dropout_rate, dense_size)\n",
    "dgaModel.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get an idea on the models inputs and outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 50, 40) <dtype: 'float32'>\n",
      "(None, 2) <dtype: 'float32'>\n",
      "input_1 [(None, 50, 40)] float32\n",
      "bidirectional (None, 50, 40) float32\n",
      "capsule_layer (None, 50, 512) float32\n",
      "flatten (None, 2, 16) float32\n",
      "dropout (None, 32) float32\n",
      "dense (None, 32) float32\n",
      "flatten_1 (None, 20) float32\n",
      "dense_1 (None, 20) float32\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None, None, None, None, None, None, None, None]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[print(i.shape, i.dtype) for i in dgaModel.inputs]\n",
    "[print(o.shape, o.dtype) for o in dgaModel.outputs]\n",
    "[print(l.name, l.input_shape, l.dtype) for l in dgaModel.layers]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compiling and fitting model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = Adam(learning_rate=0.003)\n",
    "dgaModel.compile( loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting to train models...\n",
      "Epoch 1/20\n",
      " 45/838 [>.............................] - ETA: 7:52 - loss: 0.4055 - accuracy: 0.8325"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-62-54777244a7e2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Starting to train models...\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mdgaModel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mmc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreduce_lr_acc\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\.conda\\envs\\IITMLsesh\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    106\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    107\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 108\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    109\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m     \u001b[1;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\IITMLsesh\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1096\u001b[0m                 batch_size=batch_size):\n\u001b[0;32m   1097\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1098\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1099\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1100\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\IITMLsesh\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    778\u001b[0m       \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    779\u001b[0m         \u001b[0mcompiler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"nonXla\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 780\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    781\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    782\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\IITMLsesh\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    805\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    806\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 807\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    808\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    809\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\IITMLsesh\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2827\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2828\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2829\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2830\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2831\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\IITMLsesh\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[1;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1841\u001b[0m       \u001b[0;31m`\u001b[0m\u001b[0margs\u001b[0m\u001b[0;31m`\u001b[0m \u001b[1;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1842\u001b[0m     \"\"\"\n\u001b[1;32m-> 1843\u001b[1;33m     return self._call_flat(\n\u001b[0m\u001b[0;32m   1844\u001b[0m         [t for t in nest.flatten((args, kwargs), expand_composites=True)\n\u001b[0;32m   1845\u001b[0m          if isinstance(t, (ops.Tensor,\n",
      "\u001b[1;32m~\\.conda\\envs\\IITMLsesh\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1921\u001b[0m         and executing_eagerly):\n\u001b[0;32m   1922\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1923\u001b[1;33m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[0;32m   1924\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0;32m   1925\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[1;32m~\\.conda\\envs\\IITMLsesh\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    543\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    544\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 545\u001b[1;33m           outputs = execute.execute(\n\u001b[0m\u001b[0;32m    546\u001b[0m               \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    547\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\IITMLsesh\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     57\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 59\u001b[1;33m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[0;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[0;32m     61\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print(\"Starting to train models...\")\n",
    "dgaModel.fit(X_train, y_train, validation_data=(X_test, y_test), batch_size=batch_size, epochs=20, verbose = 1, callbacks=[mc, reduce_lr_acc])"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "463e569dd19ea3c226f6c8f6e67df6be81ddf2551be5ba537ffeabae906a0f83"
  },
  "kernelspec": {
   "display_name": "Python 3.8.11 64-bit ('IITMLsesh': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
